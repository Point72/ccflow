{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "534d9832-8e0d-41ee-a49a-8bf693f1a27e",
   "metadata": {
    "tags": []
   },
   "source": [
    "# `ccflow` Tutorial"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c0d968d9-0169-47a3-a0e3-60fa543200bc",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Introduction"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The `ccflow` framework is a collection of tools and patterns for workflow configuration and orchestration.\n",
    "Its intended uses include ETL, data validation, model training, live trading configuration, backtesting, hyperparameter search, and automated report generation.\n",
    "\n",
    "The framework provides \n",
    " - a way to to manage hierarchical, strongly typed configurations and the relationships between them through composition\n",
    " - a way to associate user-defined functions with configurations to define and name configurable workflow graphs\n",
    " - flexibility in how to interact with configurations and workflows, including files/command line, native python/Jupyter notebook, Airflow/job scheduler, REST API, etc  (in progress)\n",
    "\n",
    "In this tutorial, we walk through the background, motivation and simple examples."
   ],
   "id": "5e761a460e4c26b2"
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b12369a0-165e-4505-bb7e-f55fb85cfffb",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Configuration Design Goals"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d988aed7-9577-4d99-8eb0-b12a268dc417",
   "metadata": {
    "tags": []
   },
   "source": [
    "In both production applications and research workflows, the need arises to configure various components. As these applications and workflows get increasingly complex, so do the patterns and frameworks that people use for configuration. While some of this complexity is unavoidable, in an ideal world, there is a single well-designed (hopefully!) framework that can be used across all use cases, spanning data retrieval, validation, transformation, and loading (i.e. ETL worfklows), model training and hyperparameter search, portfolio construction and optimization, backtesting, report generation, and live system and trading configuration. \n",
    "\n",
    "In order to meet the demands of these varying applications, the ideal configuration framework must satisy several needs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3647430e-fbcb-4efa-a6e9-036b6811fd9a",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Interactivity"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "516a5319-1b03-44f4-91e2-9ebc7d4ad293",
   "metadata": {
    "tags": []
   },
   "source": "Since the aim is to use configuration for both production applications/worflows as well as for research, there needs to be both a relatively static, well-controlled way of defining the entirety of the configuration, as well as much more dynamic ways of interacting and iterating over the configuration options. Versioned file-based configurations are almost always used to accomplish the former, and flexible command line interfaces are often used to satisfy the latter. In an ideal world it should be possible to both modify and add a completely new configurations directly from a python script or notebook for research, without having to resort to modifying files or leveraging command line overrides (though this is also useful functionality to have)."
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "292ce89b-e653-494a-9304-54fa3c61e626",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Schemas "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f26ab0f2-1201-4e91-a208-441ba7e91358",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "As the framework scales, the chances of two common types of errors increase:\n",
    " \n",
    " 1. Misnaming or mistyping of configuration options that could cause the configuration to silently fail (if the parameter was optional) i.e. \"threshold\" vs \"threshhold\"\n",
    " 2. Type errors and value constraints, i.e. the identifier string \"12345\" vs the integer 1234, or specifying that a parameter \"sigma\" should be non-negative.\n",
    " \n",
    "Typically, we want to catch these errors as soon as possible: when the configurations are loaded rather than when they are used. This also allows for writing testing of configurations that is decoupled from testing the logic that depends on the configurations, making it easier to spot issues quickly and easily. \n",
    "\n",
    "In order to solve these issues, configurations need strongly typed schemas, with the option to perform additional (and custom) validations. There may be a need to coerce values (i.e. if the string \"1234\" is passed to a parameter that expects an int, it may be desirable to coerce it to 1234), and additional validation may be needed on the entire structure (to test validity of combinations of parameters rather than just parameters themselves). The use of schemas also means there must be a way to evolve the schema over time (adding and removing attributes), and even to version it if necessary.  \n",
    "\n",
    "In `ccflow`, we leverage the power of the very popular [pydantic](https://pydantic-docs.helpmanual.io/) library to tackle these issues, with some additional extensions. Note that while python's [dataclasses](https://docs.python.org/3/library/dataclasses.html) solve the misnaming/mistyping problem, they do not provide type checking or additional run-time validation. One can think of pydantic as a powerful extension of dataclasses which does."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9a087080-9e98-4f43-aaaa-de103e7f86aa",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Hierarchy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "461a37d1-8645-453d-8c4d-ce421a4bb123",
   "metadata": {
    "tags": []
   },
   "source": [
    "Quantitative workflows are typically very hierarchical in nature. For example, portfolio construction depends on multiple signals, and each signal may depend on its own techniques and data sources, and each technique will have its own configuration parameters, and each data source will also have parameters that configure how it was cleaned/transformed and how to access it. Thus, the configuration framework must have a modular and hierarchical structure, which means that entire parts of the hierarchy must be easy to add and remove without affecting the rest of the configuration. In a file-based representation, this means that the configuration should be spreadable across multiple files spanning several sub-directories. The interactive representation of these configs must mirror the same kind of structure. \n",
    "\n",
    "Furthermore, the hierarchy of configurations can have complex dependencies, forming a graph structure, rather than a simple tree. For example, a data source may be configured to be transformed in a particular way, and then used in multiple signals, which are then all used as part of portfolio construction. If changing the configuration of the data source, it is then important that all the signals pick up this change. The challenge lies in defining this graph structure both statically (i.e. for trading) as well as dynamically in the python code (for research).\n",
    "\n",
    "Lastly, there should ideally be a way to automatically map a piece of configuration to the code that decides how to use it. Without this, the configuration can end up acting like a large catalog of global variables that proliferate throughout the codebase, with all the same drawbacks as global variables (including increased coupling between everything). So, each piece of configuration should get used by as few high-level pieces of code as possible, rather than by multiple low-level pieces fo code. We tackle this problem by frequently binding together the configuration parameters and the code which uses the configuration in a single object.\n",
    "\n",
    "In `ccflow`, we leverage the power of Meta's [hydra](https://hydra.cc/) library for file-based and command line configuration management, but add some of our own functionality to support the interactive configuration use case. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e3bfd720-6753-4622-9598-92b487208abc",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Workflow Design Goals"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2cffaa1e-9744-449f-9b9e-e057a1c24d90",
   "metadata": {
    "tags": []
   },
   "source": [
    "We define a \"workflow\" solution to mean a library to help define and run a collection of inter-dependent tasks (or steps). We can break this down further into separate components\n",
    " 1. Defining (via configuration) what tasks/steps make up the workflow\n",
    " 2. Passing data between tasks, so that each task has the information that it needs from upstream tasks\n",
    " 3. Determining the order in which to run the tasks (often referred to scheduling, or more appropriately \"task scheduling\")\n",
    " 4. Automating the launch of the workflow so that it runs regularly according to some rules (also referred to as scheduling, or more appropriately \"workflow scheduling\")\n",
    " 5. Advanced features such as caching, distributed evaluation, monitoring, UI's, etc\n",
    " \n",
    "There are numerous existing packages and products in the Python ecosystem which tackle the problem of workflow management, each written with different use cases in mind and supporting different sets of requirements and features. As a result\n",
    " * It is best for us to have a layer in between the business logic and any specific framework\n",
    " * It is important to define our high level requirements\n",
    " \n",
    "Even within Cubist, there is no definitive answer to the first question, so we attempt to lay out the goals from a CCRT perspective, with the understanding that we may not meet everyone's use cases.\n",
    "\n",
    "One might argue that most of the existing open source solution to this problem tend to focus on (or are marketed on) the later elements in the list above rather than the earlier ones. Our approch is to focus on the components roughly in the order they are listed."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2d22e07d-5d84-489e-9ed3-5cc71abf472f",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Ease of Use"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d6bc7a51-00a3-4019-a071-cf24141a0e6b",
   "metadata": {
    "tags": []
   },
   "source": [
    "As much as possible, it should be easy and intuitive to define workflows in the framework. Simple things should be easy, but arbitrarily complex things should remain possible. \n",
    "\n",
    "Furthermore, we should not impose too many constraints on how users write their code - they should be able to bring their existing analytics, no matter what underlying python packages or tools they use, and hook it into the framework. \n",
    "\n",
    "We aim to leverage standard and familiar programming paradigms as much as possible (writing objects and functions), as they are time-tested and easy for users to understand. By using composition of classes and functions and their return values, the python language essentially handles items 2. and 3. above for us, without a need to do anything special or for users to learn anything new.\n",
    "\n",
    "We are not trying to write a new language that people have to learn in order to implement their analytics. However, the framework should support workflow steps that use any such \"language\" that already exists (i.e. tensorflow/pytorch/jax/csp/etc).\n",
    "\n",
    "We do not wish to make assumptions about how data is represented within the framework, or even that all data should be tabular or array-like; we should be able to support documents, charts, event streams or any other kinds of objects as part of the workflows. At the same time, we should be able to offer common tools (within the framework) to facilitate common tabular data processing tasks (i.e. reading and writing).\n",
    "\n",
    "Once defined/configured, we would like a workflow to have a very simple way to run it, whether interactively or from command line. Furthermore, it should be equally easy to run any intermediate step of a workflow (and it's dependencies) to promote reusability and make debugging easy."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "53402f68-d7ca-4d52-a57b-3adc55346532",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Configuration"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "37b1aa13-c0e7-454a-a1da-b052bb3e26b7",
   "metadata": {
    "tags": []
   },
   "source": [
    "Given a configuration framework meeting the design goals laid out above, suitable for both production and research configuration, a key requirement is to be able to configure workflows using the same framework. \n",
    "\n",
    "This implies that workflows should be easy to define from version configs (i.e. using files) in production, or to change and re-run interactively from python (for research). Thus, the worfklow, the steps in the workflow, and the objects used by those steps will all belong to the same configuration paradigm."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9e34b648-d822-499f-b8a8-6759e0ff20e0",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Parameterization"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d0412b43-ec04-4762-8128-5171c1b99622",
   "metadata": {
    "tags": []
   },
   "source": [
    "While the configuration framework allows for arbitrary complexity in the configuration and definition of the workflow steps (and thus of the workflow), it is often natural to thinkof the workflows as being parameterized (or templatized) across certain dimensions, and to treat these \"context\" parameters separately from other configuration options to make it easier to run multiple templatized workflows without needing to re-configure anything. \n",
    "\n",
    "For example, many data processing workflows are parameterized by date. However, this is not the only option; in many cases it is more efficient to process data across a date range. Taking the idea further, one may also want to specify a workflow that applies to a specific region and time range, or even down to a symbol and date range level. In the realm of data orchestration workflows, another way to think about the \"context\" is as the definition of the smallest \"chunk\" of data we are willing to operate on in a step.  \n",
    "\n",
    "Thus, we want to be able to define a flexible, parametric \"context\" for each step, such that the step can be easily run across multiple contexts, and depend on other steps, each of which may use the same context or even a different context.\n",
    "\n",
    "A technical reason for parameterizing the steps separately from the configuration is to prevent run-time mutation of the configuration, which is dangerous as configurations are shared across multiple components. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e4a9d961-0467-4612-8623-dcf23d6f982c",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Configuration with `ccflow`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "93da5ef1-b4d2-47be-ac81-756da5fd8b43",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Config Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a9ec079-fbcd-4f92-88b2-523b3e585457",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from IPython.display import JSON\n",
    "from ccflow import BaseModel, ModelRegistry\n",
    "from datetime import date\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "from typing import List\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5ad6b8c2-5d33-4339-bed8-52f5a135ba20",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Basic Config with BaseModel\n",
    "\n",
    "Let's get started with some very simple examples. Pydantic calls their classes \"Models\", and so we use the same terminology; think of a \"Model\" as a \"Configurable\" class.\n",
    "\n",
    "The `BaseModel` is our base class for all configuration. We have subclassed Pydantic's `BaseModel` to change some of the default configuration options, and to make the objects play nicer with Hydra and the rest of our framework. However, as they are still Pydantic models, everything you can do with pydantic's [Models](https://pydantic-docs.helpmanual.io/usage/models/) can be done with these. \n",
    "\n",
    "We begin with a dummy example, but one which illustrates how new config schemas and values can be easily defined and manipulated on-the-fly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd5191cd-5c01-4a54-96fd-1144495cd9dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyFileConfig(BaseModel):\n",
    "    \"\"\"This is an example of a config class.\"\"\"\n",
    "\n",
    "    file: Path\n",
    "    description: str = \"N/A\"\n",
    "    version: int = 0"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0c4b9c9b-afcf-4a34-81e5-ee32b86ec0db",
   "metadata": {},
   "source": [
    "This is not very exciting yet, basically just the definition of schema configs, but it already illustrates how they can be nested and we can show how pydantic will conform input data to the right types (i.e. Path, str and int in this case)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6efe1b3c-51b4-4744-8e83-ce8acfb8964f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MyFileConfig(file=PosixPath('Tutorial.ipynb'), description='N/A', version=1)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = MyFileConfig(file=\"./Tutorial.ipynb\", version=\"1\")\n",
    "c"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e8bf61c7-64ff-423f-aa6e-cb80b82bfad5",
   "metadata": {},
   "source": [
    "Note that the config object is mutable by default (though they can be frozen too). This makes it easy to change configs, especially once they get nested"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3efd45f7-05a7-4ff1-87e7-94b56d999ab0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MyFileConfig(file=PosixPath('Tutorial.ipynb'), description='Flow example notebook', version=1)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.description = \"Flow example notebook\"\n",
    "c"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f5793508-0a3b-4479-9a43-cb99a595c7c4",
   "metadata": {},
   "source": [
    "Pydantic allows for objects to be created directly from dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "be40d8d4-81b6-41e7-8d55-5efdf4116e77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MyFileConfig(file=PosixPath('Tutorial.ipynb'), description='N/A', version=1)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = {\"file\": \"./Tutorial.ipynb\", \"version\": \"1\"}\n",
    "MyFileConfig.parse_obj(config)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d081cd84-e54d-44bc-915f-80829d7323c2",
   "metadata": {},
   "source": [
    "Pydantic provides a [JSON schema ](https://json-schema.org/) in standardized format that can users understand the parameters on the config object, though this only works on models that only contain json-compatible types (even though pydantic supports arbitrary types as we will see later). For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f8890ec2-e6e7-4ca9-9061-3c07ad0d45ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "additionalProperties": false,
       "description": "This is an example of a config class.",
       "properties": {
        "description": {
         "default": "N/A",
         "title": "Description",
         "type": "string"
        },
        "file": {
         "format": "path",
         "title": "File",
         "type": "string"
        },
        "version": {
         "default": 0,
         "title": "Version",
         "type": "integer"
        }
       },
       "required": [
        "file"
       ],
       "title": "MyFileConfig",
       "type": "object"
      },
      "text/plain": [
       "<IPython.core.display.JSON object>"
      ]
     },
     "execution_count": 6,
     "metadata": {
      "application/json": {
       "expanded": false,
       "root": "root"
      }
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "JSON(MyFileConfig.schema())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "458cbe02-37d3-417c-b427-c5a56add8f59",
   "metadata": {},
   "source": [
    "Pydantic's type validation will catch cases that are incompatible with our schema definition. In fact, pydantic can be used to place even greater constraints on the values themselves (i.e. version must be positive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9be59428-3734-445c-a211-d8f0e9167d4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 validation error for MyFileConfig\n",
      "version\n",
      "  Input should be a valid integer, unable to parse string as an integer [type=int_parsing, input_value='foo', input_type=str]\n",
      "    For further information visit https://errors.pydantic.dev/2.7/v/int_parsing\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    c.version = \"foo\"\n",
    "except ValueError as e:\n",
    "    print(e)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c0f14f1c-e529-4fff-af71-05e86615ac1c",
   "metadata": {},
   "source": [
    "Furthermore, we have enabled the option by default to raise exceptions when field names are mis-specified (or extra fields are provided) to catch potential configuration mistakes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dc10aedd-e537-4298-8f7e-88798ee5dc76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 validation error for MyFileConfig\n",
      "Version\n",
      "  Extra inputs are not permitted [type=extra_forbidden, input_value=1, input_type=int]\n",
      "    For further information visit https://errors.pydantic.dev/2.7/v/extra_forbidden\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    MyFileConfig(file=\"./Tutorial.ipynb\", Version=1)\n",
    "except ValueError as e:\n",
    "    print(e)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3e6a0a84-648b-432b-bc1c-ee494dcee3c6",
   "metadata": {},
   "source": [
    "### Hierarchical Configs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "381d5559-0b9c-4fbf-a050-7c9081aabdaf",
   "metadata": {},
   "source": [
    "Hierarchical configs are also easy to work with. Below, we create a new config which consists of two file configs, and we can easily modify nested attributes using standard python syntax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "782d6335-d570-4801-b133-77d2a477cc75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "$defs": {
        "MyFileConfig": {
         "additionalProperties": false,
         "description": "This is an example of a config class.",
         "properties": {
          "description": {
           "default": "N/A",
           "title": "Description",
           "type": "string"
          },
          "file": {
           "format": "path",
           "title": "File",
           "type": "string"
          },
          "version": {
           "default": 0,
           "title": "Version",
           "type": "integer"
          }
         },
         "required": [
          "file"
         ],
         "title": "MyFileConfig",
         "type": "object"
        }
       },
       "additionalProperties": false,
       "description": "This is the example of a nested config.",
       "properties": {
        "data_one": {
         "allOf": [
          {
           "$ref": "#/$defs/MyFileConfig"
          }
         ],
         "default": null
        },
        "data_two": {
         "allOf": [
          {
           "$ref": "#/$defs/MyFileConfig"
          }
         ],
         "default": null
        }
       },
       "title": "MyDataConfig",
       "type": "object"
      },
      "text/plain": [
       "<IPython.core.display.JSON object>"
      ]
     },
     "execution_count": 9,
     "metadata": {
      "application/json": {
       "expanded": false,
       "root": "root"
      }
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MyDataConfig(BaseModel):\n",
    "    \"\"\"This is the example of a nested config.\"\"\"\n",
    "\n",
    "    data_one: MyFileConfig = None\n",
    "    data_two: MyFileConfig = None\n",
    "\n",
    "\n",
    "JSON(MyDataConfig.schema())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8404fe81-03d2-44bd-8b93-356bd9f21db2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python init file: ccflow/__init__.py\n"
     ]
    }
   ],
   "source": [
    "c2 = MyDataConfig(\n",
    "    data_one=c,\n",
    "    data_two=MyFileConfig(file=\"ccflow/__init__.py\"),\n",
    ")\n",
    "c2.data_two.description = \"Python init file\"\n",
    "print(f\"{c2.data_two.description}: {c2.data_two.file}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ae5dc818-0314-43de-94b3-2ae18c463b7c",
   "metadata": {},
   "source": [
    "Pydantic also provides the ability to coerce dictionaries recursively into structured types, so long as the types have been declared on the schema. For example, it will automatically create the `MyFileConfig` instance if we just pass a dictionary to `MyDataConfig`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5299644c-7ef3-4b1a-97e9-fe31f9c3ced3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MyDataConfig(data_one=MyFileConfig(file=PosixPath('Tutorial.ipynb'), description='Flow example notebook', version=0), data_two=None)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MyDataConfig(\n",
    "    data_one={\n",
    "        \"file\": \"./Tutorial.ipynb\",\n",
    "        \"description\": \"Flow example notebook\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c649cb73-dd94-4552-b9bb-0e243e39f1a3",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Config Inheritance and Templatization"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5df81baa-5cfc-4b6d-a3f4-60cef3effbdb",
   "metadata": {
    "tags": []
   },
   "source": [
    "In addition to the composition of configs as illustrated above, there may be cases when inheritance and templatization is required. Pydantic supports both of these out of the box. \n",
    "\n",
    "Below we provide an example of multiple inheritence of model objects, which further illustrates the power of having schema classes for configuration over raw dictionaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5f47dbf6-f8c8-43f8-9818-f8c5fe1c6caa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MyConfig(universe='US', start_date=datetime.date(2022, 1, 1), end_date=datetime.date(2023, 1, 1), parameter=4)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class DateRangeMixin(BaseModel):\n",
    "    start_date: date\n",
    "    end_date: date\n",
    "\n",
    "\n",
    "class UniverseMixin(BaseModel):\n",
    "    universe: str\n",
    "\n",
    "\n",
    "class MyConfig(DateRangeMixin, UniverseMixin, BaseModel):\n",
    "    parameter: int\n",
    "\n",
    "\n",
    "MyConfig(\n",
    "    parameter=4, universe=\"US\", start_date=date(2022, 1, 1), end_date=date(2023, 1, 1)\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a47e998d-4757-4300-bd9d-64d17be7919e",
   "metadata": {
    "tags": []
   },
   "source": [
    "For examples of templatization, refer to the section of the pydantic documentation on [Generic Models](https://pydantic-docs.helpmanual.io/usage/models/#generic-models)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "15c0f701-ae2c-4b16-9435-07a6a562cd4c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Registering Configurations"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "db3a0ba8-a955-4327-b61a-8df7d9550937",
   "metadata": {
    "tags": []
   },
   "source": [
    "### The Model Registry"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8a7194bb-38bc-49a0-a6ff-0926e542cb4f",
   "metadata": {
    "tags": []
   },
   "source": [
    "`ccflow.flow` provides a `ModelRegistry` class which represents a collection of models (configs). Later we will see how config files can be mapped to a registry, but for now we illustrate how it can be used interactively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ca20bbb9-3d8a-4c47-8085-af36a6709248",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModelRegistry(name='My Raw Data')\n",
      "mappingproxy({'flow': MyFileConfig(file=PosixPath('Tutorial.ipynb'), description='Flow example notebook', version=0),\n",
      "              'init': MyFileConfig(file=PosixPath('ccflow/__init__.py'), description='Python init file', version=0)})\n"
     ]
    }
   ],
   "source": [
    "r = ModelRegistry(name=\"My Raw Data\")\n",
    "r.add(\n",
    "    \"flow\",\n",
    "    MyFileConfig(\n",
    "        file=\"Tutorial.ipynb\", description=\"Flow example notebook\"\n",
    "    ),\n",
    ")\n",
    "r.add(\n",
    "    \"init\",\n",
    "    MyFileConfig(\n",
    "        file=\"ccflow/__init__.py\", description=\"Python init file\"\n",
    "    ),\n",
    ")\n",
    "print(r)\n",
    "pprint(r.models)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4ae102d8-d562-4f9e-8981-59192748cf77",
   "metadata": {},
   "source": [
    "At this point, a `ModelRegistry` just looks and behaves like a dictionary. However, a bit of extra functionality has been built in, such as validation of items that go into the registry to make sure they are config classes, i.e."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "af6de3d0-bc5a-4b59-890a-abbc931ff0e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model must be a child class of <class 'ccflow.base.BaseModel'>, not '<class 'dict'>'.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    r.add(\"bad_data\", {\"foo\": 5, \"bar\": 6})\n",
    "except TypeError as e:\n",
    "    print(e)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "36e1a6d0-8aa9-4669-98f7-5e268231033c",
   "metadata": {},
   "source": [
    "This may seem like an unnecessary restriction, but enforcing that all registry elements are using BaseModel means that we can deliver more powerful functionality over time by extending the BaseModel implementation."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b51e63f5-fabf-4b84-a98b-bdc12167e814",
   "metadata": {},
   "source": [
    "As the amount of configuration grows, there is a desire to organize these objects in a hierarchy, and so, the registry class can contain other registries (since they are configuration objects themselves).\n",
    "\n",
    "Furthermore, instead of passing various registries around in the code, it is sometimes helpful to have a single registry that is a singleton at the \"root\" of all these registries. `ccflow` provides this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "67cd7ec1-f64f-467c-887e-47f0dbc3d6e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "RootModelRegistry()\n",
      "{'raw data': ModelRegistry(name='My Raw Data')}\n"
     ]
    }
   ],
   "source": [
    "root = ModelRegistry.root()\n",
    "print(root is ModelRegistry.root())  # It is a singleton.\n",
    "root.add(\"raw data\", r, overwrite=True)\n",
    "print(root)\n",
    "print(root.models)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "640ff486-a902-454f-a507-90a217e8dd4e",
   "metadata": {},
   "source": [
    "From the root registry, there are three diffent ways to get underlying configs, using dictionary syntax, file path or getter syntax:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1df6d171-b3d6-4ecd-9f39-d5214b8d94fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MyFileConfig(file=PosixPath('Tutorial.ipynb'), description='Flow example notebook', version=0)\n",
      "MyFileConfig(file=PosixPath('Tutorial.ipynb'), description='Flow example notebook', version=0)\n",
      "MyFileConfig(file=PosixPath('Tutorial.ipynb'), description='Flow example notebook', version=0)\n"
     ]
    }
   ],
   "source": [
    "print(root[\"raw data\"][\"flow\"])  # Dictionary syntax\n",
    "print(root[\"raw data/flow\"])  # File path syntax\n",
    "print(root.get(\"raw data\").get(\"flow\")) # Getter syntax"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "739bbfed-a4c4-43a2-8d69-da5951377aac",
   "metadata": {},
   "source": [
    "Note that the same object can be registered under multiple different names. If one thinks of a registry as a \"catalog\" of data or configurations, it makes sense that the same item could be indexed in different ways. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "65e35d96-2816-4e30-ad0d-19e499cc8d02",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Dependencies"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "24bc55d3-1e9a-46b9-90b6-9dc115fefa9f",
   "metadata": {},
   "source": [
    "As mentioned in the Introduction, we wish to allow configuration objects to depend on each other, in such a way that the linkage is dynamic. In the `ccflow` framework, this is done through object composition (and mutability of configs). However, to make things easier, we allow for configs to be referenced by their name in the **root** registry! For example, we can create a new config object like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4ee68038-0099-49a9-aed9-610fb1d35ac2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MyDataConfig(data_one=MyFileConfig(file=PosixPath('Tutorial.ipynb'), description='Flow example notebook', version=0), data_two=None)\n"
     ]
    }
   ],
   "source": [
    "root = ModelRegistry.root()\n",
    "root.add(\"raw data\", r, overwrite=True)\n",
    "\n",
    "new_config = MyDataConfig(data_one=\"raw data/flow\")\n",
    "print(new_config)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8056a2c4-68f3-4af4-8472-86ad9ec81ed7",
   "metadata": {},
   "source": [
    "If we now change the values on this config model in the registry, they will change in this newly created object as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6b7ec3cc-a5ab-4b59-b438-a372a9c2e614",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Flow notebook\n"
     ]
    }
   ],
   "source": [
    "root[\"raw data\"][\"flow\"].description = \"New Flow notebook\"\n",
    "print(new_config.data_one.description)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4c186152-a380-4173-93c9-336195e69b26",
   "metadata": {},
   "source": [
    "Note, however, that if we replace the config object in the registry with an entirely new config object, the dependency will still reference the old object. This is why you need to pass `overwrite=True` when adding an object to the registry with a name that already exists\n",
    "\n",
    "With these dependencies set up, we can now register this object as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9718f6d3-adbb-4fc9-a1be-cfd59e02ce91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'data config': MyDataConfig(data_one=MyFileConfig(file=PosixPath('Tutorial.ipynb'), description='New Flow notebook', version=0), data_two=None),\n",
      " 'raw data': ModelRegistry(name='My Raw Data')}\n"
     ]
    }
   ],
   "source": [
    "root.add(\"data config\", new_config, overwrite=True)\n",
    "pprint(root.models.copy())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5cfd6021-d5d1-4df2-be9a-2a768cdd6c2c",
   "metadata": {},
   "source": [
    "Even once registered, linkages between objects can be added through simple assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0f3b24fd-d6a5-4113-b32a-c8509f087293",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Python init file'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "root[\"data config\"].data_two = \"/raw data/init\"\n",
    "root[\"data config\"].data_two.description"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d1d0712a-114a-41a5-8b92-9e686cdd0ae8",
   "metadata": {},
   "source": [
    "The config objects in `ccflow` can tell you where they are registered (which may be in more than one place), either as a tuple of (registry, name), or as a path by which the object could be accessed. i.e. For the composite configuration `\"data config\"`, registered in the root registry:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f811c443-32f8-47f5-b315-b64fb53f139e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(RootModelRegistry(), 'data config')]\n",
      "['/data config']\n"
     ]
    }
   ],
   "source": [
    "print(new_config.get_registrations())\n",
    "print(new_config.get_registered_names())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "272e22cc-8f95-49e2-a583-7db356e9d08e",
   "metadata": {},
   "source": [
    "Below is an example of that for the calendar config, as accessed from the data config:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3931a764-0c61-4089-8193-d11ee622e418",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(ModelRegistry(name='My Raw Data'), 'init')]\n",
      "['/raw data/init']\n"
     ]
    }
   ],
   "source": [
    "print(root[\"data config\"].data_two.get_registrations())\n",
    "print(root[\"data config\"].data_two.get_registered_names())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "284e84c6-5fb5-4b27-90f7-ac15f8861d12",
   "metadata": {},
   "source": [
    "The config objects can also tell you their dependencies on other registered config objects. It will look recursively through the the entire nested configuration structure to find other models that are in the registry (even if some intermediate levels are not registered):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "34151a03-1a7d-462e-8eca-f41342b9c767",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['/raw data/flow'], ['/raw data/init']]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_config.get_registry_dependencies()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9468a35d-d3b4-4de6-8e5e-894e76d525b7",
   "metadata": {},
   "source": [
    "To clear the root from all it's entries and start over, one can execute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1fd5f8b1-5e34-41ec-a291-9d1b14ef4716",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n"
     ]
    }
   ],
   "source": [
    "root.clear()\n",
    "print(root.models)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8415efae-6d04-470d-9ffe-760fe6485f5d",
   "metadata": {},
   "source": [
    "When you do this, the registrations and dependencies on previously registered objects are also reset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b6557b46-c340-4c25-8dbb-e287f2ee4cd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(new_config.get_registrations())\n",
    "print(new_config.get_registry_dependencies())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3af8ce47-b080-4a2d-a7ae-3de84ad11832",
   "metadata": {},
   "source": [
    "### To/From Dictionaries"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e05a1da3-000a-45d4-addf-fcf39a3a6a1b",
   "metadata": {},
   "source": [
    "Often times, configuration objects start out as dictionaries, and need to be mapped into BaseModels for entry into the registry. While pydantic can do conversion (and validation) of dictionaries to models when the type is known, often the type is part of the configuration itself (i.e. as a string that represents the path to the object). As we leverage Hydra for file-based configuration, they adopt their convention that a field named `_target_` on the config can be used to represent the class type. \n",
    "\n",
    "We can then use hydra's utilities to help create the config objects:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "59df085c-f99b-4404-9fde-c8e6acdf981b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MyFileConfig(file=PosixPath('Tutorial.ipynb'), description='Flow example notebook', version=0)\n"
     ]
    }
   ],
   "source": [
    "from hydra.utils import instantiate\n",
    "\n",
    "config = instantiate(\n",
    "    {\n",
    "        \"_target_\": \"__main__.MyFileConfig\",\n",
    "        \"file\": \"Tutorial.ipynb\",\n",
    "        \"description\": \"Flow example notebook\",\n",
    "    }\n",
    ")\n",
    "print(config)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ace8c4e8-fe5e-4078-a363-c9c9cff93214",
   "metadata": {},
   "source": [
    "Pydantic provides the ability to serialize models into dictionaries (or json), and the `ccflow.BaseModel` allows for serialization of the config attributes along with its type (as the `_target_`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cc86b445-70f4-4e05-9c05-44751da51faf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'file': PosixPath('Tutorial.ipynb'),\n",
       " 'description': 'Flow example notebook',\n",
       " 'version': 0,\n",
       " '_target_': '__main__.MyFileConfig'}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config.dict(by_alias=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f645f99a-9f96-48a5-92b6-a7aa621263f6",
   "metadata": {},
   "source": [
    "Configurations can also be serialized to json (so long as all elements are themselves json serializable, though pydantic allows for user-specified json serializers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "154a5fb9-d562-4770-a2ce-45078bb3d9b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"file\":\"Tutorial.ipynb\",\"description\":\"Flow example notebook\",\"version\":0,\"_target_\":\"__main__.MyFileConfig\"}'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config.json(by_alias=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "48602ad4-f719-45cc-a891-70d2595ab28c",
   "metadata": {},
   "source": [
    "We have a utility function to make it easier to view the object as JSON in a Jupyter notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "15974508-92dd-4958-8f7f-38a14ca3ff13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "_target_": "__main__.MyFileConfig",
       "description": "Flow example notebook",
       "file": "Tutorial.ipynb",
       "version": 0
      },
      "text/plain": [
       "<IPython.core.display.JSON object>"
      ]
     },
     "execution_count": 29,
     "metadata": {
      "application/json": {
       "expanded": false,
       "root": "root"
      }
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config.get_widget()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "172c1766-129d-4db7-853b-5b0ce17049dd",
   "metadata": {},
   "source": [
    "Thus, by adding a `_target_` value whenever the type is not known (usually at the root of the structure), pydantic and hydra combined can easily de-serialize (and validate) nested configurations as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "42a9097c-4706-4f26-956c-9669b9c78b01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "_target_": "__main__.MyDataConfig",
       "data_one": {
        "_target_": "__main__.MyFileConfig",
        "description": "Flow example notebook",
        "file": "Tutorial.ipynb",
        "version": 1
       },
       "data_two": {
        "_target_": "__main__.MyFileConfig",
        "description": "Python init file",
        "file": "ccflow/__init__.py",
        "version": 0
       }
      },
      "text/plain": [
       "<IPython.core.display.JSON object>"
      ]
     },
     "execution_count": 30,
     "metadata": {
      "application/json": {
       "expanded": true,
       "root": "root"
      }
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config_dict = {\n",
    "    \"_target_\": \"__main__.MyDataConfig\",\n",
    "    \"data_one\": {\n",
    "        \"file\": \"Tutorial.ipynb\",\n",
    "        \"description\": \"Flow example notebook\",\n",
    "        \"version\": \"1\",\n",
    "    },  # Note that \"1\" will be converted to 1\n",
    "    \"data_two\": {\n",
    "        \"file\": \"ccflow/__init__.py\",\n",
    "        \"description\": \"Python init file\",\n",
    "        \"version\": 0,\n",
    "    },\n",
    "}\n",
    "config = instantiate(config_dict)\n",
    "config.get_widget(widget_kwargs={\"expanded\":True})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "76904985-41b3-4d87-a428-094a6ab79ef8",
   "metadata": {},
   "source": [
    "In fact, the `ModelRegistry` class has a convenience method that will take a dictionary of configs, and add them all to the registry. It is even clever enough to interpret nested dictionaries (with no `_target_`) as nested registries, and to resolve dependencies specified as strings (by looking up the string path in the **root** registry). Below is a complete example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "597227ee-1664-498d-a311-042fbd1c084f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "_target_": "ccflow.base.RootModelRegistry",
       "models": {
        "data config": {
         "_target_": "__main__.MyDataConfig",
         "data_one": {
          "_target_": "__main__.MyFileConfig",
          "description": "Flow example notebook",
          "file": "Tutorial.ipynb",
          "version": 1
         },
         "data_two": {
          "_target_": "__main__.MyFileConfig",
          "description": "Python init file",
          "file": "ccflow/__init__.py",
          "version": 0
         }
        },
        "raw data": {
         "_target_": "ccflow.base.ModelRegistry",
         "models": {
          "flow": {
           "_target_": "__main__.MyFileConfig",
           "description": "Flow example notebook",
           "file": "Tutorial.ipynb",
           "version": 1
          },
          "init": {
           "_target_": "__main__.MyFileConfig",
           "description": "Python init file",
           "file": "ccflow/__init__.py",
           "version": 0
          }
         },
         "name": "raw data"
        }
       },
       "name": ""
      },
      "text/plain": [
       "<IPython.core.display.JSON object>"
      ]
     },
     "execution_count": 31,
     "metadata": {
      "application/json": {
       "expanded": true,
       "root": "root"
      }
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_configs = {\n",
    "    \"raw data\": {\n",
    "        \"flow\": {\n",
    "            \"_target_\": \"__main__.MyFileConfig\",\n",
    "            \"file\": \"Tutorial.ipynb\",\n",
    "            \"description\": \"Flow example notebook\",\n",
    "            \"version\": \"1\",\n",
    "        },\n",
    "        \"init\": {\n",
    "            \"_target_\": \"__main__.MyFileConfig\",\n",
    "            \"file\": \"ccflow/__init__.py\",\n",
    "            \"description\": \"Python init file\",\n",
    "            \"version\": 0,\n",
    "        },\n",
    "    },\n",
    "    \"data config\": {\n",
    "        \"_target_\": \"__main__.MyDataConfig\",\n",
    "        \"data_one\": \"raw data/flow\",\n",
    "        \"data_two\": \"raw data/init\",\n",
    "    },\n",
    "}\n",
    "root = ModelRegistry.root().clear()\n",
    "root.load_config(all_configs)\n",
    "root.get_widget(widget_kwargs={\"expanded\":True})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "50f8aa60-940c-4643-9752-7c80e91169d6",
   "metadata": {},
   "source": [
    "### From Files"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "209d9045-0ec5-452c-b421-32f197714001",
   "metadata": {},
   "source": [
    "With the above pieces in place, especially the ability to load dictionaries of configs (specified as dictionaries) into the root `ModelRegistry`, the next step is to be able to load configuration files into this type of structure. Fortunately, this piece has already been solved by [Hydra](https://hydra.cc/docs/intro/) (which itself depends on [OmegaConf](https://omegaconf.readthedocs.io/) under the hood), so we depend on that, rather than re-inventing the wheel. In particular, Hydra allows for configurations to be split across multiple sub-directories and files, re-used in multiple places, and recombined as needed, in addition to some advanced command line tools. \n",
    "\n",
    "While hydra is primarily concerned with loading file-based configurations into command-line applications, their [Compose API](https://hydra.cc/docs/advanced/compose_api/) provides a way to load the configs interactively in a notebook, as a special dictionary type. However, to save people the work of first loading config files into config dictionaries, and then adding those into the registry, we've provided a function to do this directly, shown below. Note that these config files are loading models which are defined in `ccflow` and `ccflow.examples`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b7572e47-98c2-4565-92e8-bd651c7fc15a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "_target_": "ccflow.base.RootModelRegistry",
       "models": {
        "sources": {
         "_target_": "ccflow.base.ModelRegistry",
         "models": {
          "other1": {
           "_target_": "ccflow.examples.flow.Other1"
          },
          "other2": {
           "_target_": "ccflow.examples.flow.Other2"
          },
          "source1": {
           "_target_": "ccflow.examples.flow.Source1FromDb",
           "conn_str": "PROD",
           "query": "select * from ..."
          },
          "source2": {
           "_target_": "ccflow.examples.flow.Source2FromDb",
           "conn_str": "PROD2",
           "query": "select * from ..."
          }
         },
         "name": "sources"
        }
       },
       "name": ""
      },
      "text/plain": [
       "<IPython.core.display.JSON object>"
      ]
     },
     "execution_count": 32,
     "metadata": {
      "application/json": {
       "expanded": false,
       "root": "root"
      }
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ccflow.examples\n",
    "root = ModelRegistry.root().clear()\n",
    "absolute_path = Path(ccflow.examples.__file__).parent / \"config/conf.yaml\"\n",
    "root.load_config_from_path(path=absolute_path, config_key=\"registry\")\n",
    "root.get_widget(widget_kwargs={\"expanded\":False})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "119b0171-b826-45e2-a687-b293d36e7c70",
   "metadata": {},
   "source": [
    "The \"config_key\" argument in the function call above points to the subset of the hydra configs to load into the registry, as there may be parts of the config which you do not which to load into the registry (such as configuration of hydra itself, or potentially other global configuration variables that are only meant to exist in the file layer). "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "792b1934-8e66-42cb-88b3-55cb2ed96b06",
   "metadata": {},
   "source": [
    "It is out-of-scope for this tutorial to cover the various ways in which hydra can be used to generate configs, but please check out their [documentation](https://hydra.cc/docs/intro/) for more information."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f1f42d56-0a82-40af-bd82-06e59d4eb19f",
   "metadata": {},
   "source": [
    "## Advanced Config Examples"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fff00b1a-49cf-407f-9139-ebb025d1b8d2",
   "metadata": {},
   "source": [
    "In this section we cover some more advanced config examples. It can be skipped on the first read if desired"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bdec2d8a-125d-437d-be7d-286f4e5e4247",
   "metadata": {},
   "source": [
    "### Custom Validation and Coercion"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "72b7688c-6124-4ba6-aee3-6073152bfcbe",
   "metadata": {},
   "source": [
    "Pydantic provides a lot of functionality for custom validation. We don't cover all of it in the tutorial, but encourage people to read the section of the pydantic docs on [validators](https://pydantic-docs.helpmanual.io/usage/validators/). \n",
    "\n",
    "However, one important piece of information is that when trying to conform data to a specific config type, pydantic will call the `validate` method on that type. \n",
    "\n",
    "This can be useful as a user, when trying to explicitly convert non-dictionary data types to a pydantic model. It is also useful as a programmer, as one can (carefully) override the `validate` method to provide custom coercions. We illustrate that below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5cad26a8-b933-45ba-beeb-b205b9474457",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NameConfig(first_name='John', last_name='Doe')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class NameConfig(BaseModel):\n",
    "    first_name: str\n",
    "    last_name: str\n",
    "\n",
    "    @classmethod\n",
    "    def validate(cls, values):\n",
    "        if isinstance(values, str):\n",
    "            names = values.split(\" \")\n",
    "            if len(names) == 2:\n",
    "                values = dict(first_name=names[0], last_name=names[1])\n",
    "        return super().validate(values)\n",
    "\n",
    "\n",
    "NameConfig.validate(\"John Doe\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cc98d004-b660-4fd3-abe0-ab14c30c3699",
   "metadata": {},
   "source": [
    "Now, when a string name is passed to a config where `NameConfig` is expected, the validate function will be called, and the data will be coerced to the correct type. We use this trick in several places in `ccflow` to improve usability, and mention it here in case others find it useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "58cbb9c1-1d19-48c9-a840-8ce080c78287",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyConfig(BaseModel):\n",
    "    name: NameConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e2137e67-a1d2-42e8-8f0d-f7920522c4b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 validation error for MyConfig\n",
      "name\n",
      "  Value error,  [type=value_error, input_value='John Doe', input_type=str]\n",
      "    For further information visit https://errors.pydantic.dev/2.7/v/value_error\n"
     ]
    }
   ],
   "source": [
    "from pydantic import ValidationError\n",
    "\n",
    "try:\n",
    "    MyConfig(name=\"John Doe\")\n",
    "except ValidationError as e:\n",
    "    print(e)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6bb6b0c1-fcaa-420a-a9f4-d6e5bba16be6",
   "metadata": {},
   "source": [
    "### Jinja Templates and SQL Queries"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1601bf4f-169f-477e-9744-a6290cfa205c",
   "metadata": {},
   "source": [
    "Another aspect of configuration that we haven't touched on so far is the need to specify a template document, and then to fill in the data. This occurs commonly when building database queries from parameters or when plugging data into an email template or HTML report. One common solution is to leverage python's string formatting capabilities, but this provides a minimal amount of validation to guard against accidents in the template definition, or malicious users (i.e. SQL injection attacks). The standard solution to this problem is to leverage [Jinja templates](https://jinja.palletsprojects.com/), which are extremely powerful (as they enable some amount of scripting inside the template itself). \n",
    "\n",
    "`ccflow` has defined a pydantic extension type that corresponds to Jinja templates, so that they can be used in configuration objects. We illustrate this below (note that unused template arguments are ignored)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9854ab39-1b56-4376-97b5-2ee043a9fe35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello {{user|upper}}, welcome to {{place}}!\n",
      "Hello FRIEND, welcome to the tutorial!\n",
      "Hello FRIEND, welcome to line 2!\n"
     ]
    }
   ],
   "source": [
    "from ccflow import JinjaTemplate\n",
    "\n",
    "\n",
    "class MyTemplateConfig(BaseModel):\n",
    "    greeting: JinjaTemplate\n",
    "    user: str\n",
    "    place: str\n",
    "\n",
    "\n",
    "config = MyTemplateConfig(\n",
    "    greeting=\"Hello {{user|upper}}, welcome to {{place}}!\",\n",
    "    user=\"friend\",\n",
    "    place=\"the tutorial\",\n",
    ")\n",
    "print(config.greeting)\n",
    "print(config.greeting.template.render(config.dict()))\n",
    "config.place = \"line 2\"\n",
    "print(config.greeting.template.render(config.dict()))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cb2327bb-9b4d-40c4-a5a4-0f6d842e44cd",
   "metadata": {},
   "source": [
    "While the above example may be useful for a templatized email or report, we provide a more complex and realistic example that illustrates how to easily configure a SQL query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "16a1544c-2101-4b8f-b8d3-ec6eed028e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date\n",
    "from pydantic import Field\n",
    "\n",
    "\n",
    "class MyQueryTemplate(BaseModel):\n",
    "    query: JinjaTemplate\n",
    "    columns: List[str]\n",
    "    where: str = \"Test\"\n",
    "    query_date: date = Field(default_factory=date.today)\n",
    "    filters: List[str] = Field(default_factory=list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a73cd843-501d-4b8f-8972-b7b6dea95b89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "select  Col1,\n",
      "\tCol2 as MyOthercol,\n",
      "\tSomeID\n",
      "from MyDatabase\n",
      "where WhereCol = 'Test'\n",
      "    and NextDate = '2024-06-27'\n",
      "    and Date >= dateadd(day,-14,'2024-06-27')\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "query = \"\"\"select  {{columns|join(\",\\n\\t\")}}\n",
    "from MyDatabase\n",
    "where WhereCol = '{{where}}'\n",
    "    and NextDate = '{{query_date}}'\n",
    "    and Date >= dateadd(day,-14,'{{query_date}}')\n",
    "    {% for filter in filters %}and {{filter}} {% endfor %}\n",
    "\"\"\"\n",
    "\n",
    "config = MyQueryTemplate(\n",
    "    query=query,\n",
    "    columns=[\"Col1\", \"Col2 as MyOthercol\", \"SomeID\"],\n",
    ")\n",
    "print(config.query.template.render(config.dict()))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e94281db-e3b7-4d50-b5d5-8fc85dc0842d",
   "metadata": {},
   "source": [
    "Now it's easy to reconfigure the query by, i.e. changing the date and adding filters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "224adf31-68e7-4e01-86cd-6fe4e64d5fe2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "select  Col1,\n",
      "\tCol2 as MyOthercol,\n",
      "\tSomeID\n",
      "from MyDatabase\n",
      "where WhereCol = 'Test'\n",
      "    and NextDate = '2022-01-01'\n",
      "    and Date >= dateadd(day,-14,'2022-01-01')\n",
      "    and SomeID IS NOT NULL and Col1 in 'blerg' \n"
     ]
    }
   ],
   "source": [
    "config.query_date = date(2022, 1, 1)\n",
    "config.filters = [\"SomeID IS NOT NULL\", \"Col1 in 'blerg'\"]\n",
    "print(config.query.template.render(config.dict()))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "eaea3a95-ace5-4839-bdae-2068c304dcab",
   "metadata": {},
   "source": [
    "### Numpy Arrays"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "002bce39-3c74-4f48-8dac-c5249a2b5361",
   "metadata": {},
   "source": [
    "Sometimes it is more convenient to work with numpy array objects instead of python lists. `ccflow` provides tools to do this easily, as shown in the following example (which conforms the input data to the declared types automatically)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0679f7f8-2856-47de-8340-4e9dfaf720ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MyNumpyConfig(my_array=array([1., 2., 3.]), my_list=[1.0, 2.0, 3.0])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ccflow import NDArray\n",
    "\n",
    "\n",
    "class MyNumpyConfig(BaseModel):\n",
    "    my_array: NDArray[np.float64]\n",
    "    my_list: List[float]\n",
    "\n",
    "\n",
    "MyNumpyConfig(my_array=[1, 2, 3], my_list=[1, 2, 3])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ab03324d-21a2-4396-a965-67216a6d8b62",
   "metadata": {},
   "source": [
    "### Custom Types"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "becb1ebb-0949-450c-ab01-68abe5116147",
   "metadata": {},
   "source": [
    "Often the need arises for configuration to create objects that are not built-in types (str, int, float, etc). Pydantic supports a number of additional types (see the [documentation](https://pydantic-docs.helpmanual.io/usage/types/) for a full list), but can also handle completely arbitrary types. We can also use hydra to instantiate these arbitrary types from the configs as well, and pydantic will validate that the created object is an instance of the desired type. Furthermore, we specify some extension types  in `ccflow` that have additional validation and functionality.\n",
    "\n",
    "First, to illustrate how custom types work, we define our own custom object type, and then a configuration object (i.e. `BaseModel`) that contains that type. To prevent accidental inclusion of custom types, pydantic must be explicitly told to include them using the Config option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4cfae42b-a7e0-46de-8bfd-86878cef178d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MyConfigWithCustomType(custom=<__main__.MyCustomType object at 0x11fafe210>)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from hydra.utils import instantiate\n",
    "\n",
    "\n",
    "class MyCustomType:\n",
    "    pass\n",
    "\n",
    "\n",
    "class MyConfigWithCustomType(BaseModel):\n",
    "    custom: MyCustomType\n",
    "\n",
    "    class Config:\n",
    "        arbitrary_types_allowed = True\n",
    "\n",
    "\n",
    "config = {\n",
    "    \"_target_\": \"__main__.MyConfigWithCustomType\",\n",
    "    \"custom\": {\"_target_\": \"__main__.MyCustomType\"},\n",
    "}\n",
    "instantiate(config)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7f01fa91-7716-435c-b48c-0e2df19cec2e",
   "metadata": {},
   "source": [
    "### Loading Objects by Path"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d01ea9a4-c26e-4629-a96e-1a0be082ae7b",
   "metadata": {},
   "source": [
    "Sometimes one needs to refer to an object that is already defined in the codebase by its import name. For example, in some cases, it can be easier easier to construct a config object in python, such as when lots of custom classes, enum types, or lambda functions are involved. For this case, `ccflow` also has a solution that is able to refer to any python object by path, as illustrated below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e03f50c4-6a55-4cbb-8560-63ef62613f59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Separator:  /\n"
     ]
    }
   ],
   "source": [
    "from ccflow import PyObjectPath\n",
    "\n",
    "\n",
    "class MyConfigWithPaths(BaseModel):\n",
    "    builtin_func: PyObjectPath = PyObjectPath(\"builtins.len\")\n",
    "    separator: PyObjectPath = PyObjectPath(\"ccflow.REGISTRY_SEPARATOR\")\n",
    "\n",
    "\n",
    "config = MyConfigWithPaths()\n",
    "assert config.builtin_func.object([1, 2, 3]) == 3\n",
    "print(\"Separator: \", config.separator.object)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4b57e1f8-4c00-47ed-b1fb-c7088e2837f8",
   "metadata": {},
   "source": [
    "At the moment, there is no type checking performed on the object type (we may add it in the future), but it will validate that the path is valid at load time to catch config issues as early as possible:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b372c246-dded-4ea9-8a7d-dd1da67a210a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 validation error for MyConfigWithPaths\n",
      "separator\n",
      "  Invalid python path: No module named 'foo.bar' [type=import_error, input_value='foo.bar', input_type=str]\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    config.separator = \"foo.bar\"\n",
    "except ValueError as e:\n",
    "    print(e)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a63fdbd7-c506-4c53-8169-6e18de6774b6",
   "metadata": {},
   "source": [
    "## Using Configuration"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "062f74c7-968c-444d-bb33-8c03ac946076",
   "metadata": {},
   "source": [
    "With the ability to define arbitrarily complex configuration structures following the examples above, now arises the question of how best to use all that config information. \n",
    "\n",
    "One option is simply to access the relevant config information from the root registry whenever it is needed in the code. This method is **fragile** and **strongly discouraged**. It is equivalent to using global variables throughout the code, which has the following problems\n",
    " - Causes very tight coupling between parts of the code, and adds dependencies everwhere on, i.e. the naming and structure of the configuration, which may need to change frequently to stay organized\n",
    " - Since every configuration option is available to every piece of code, it makes it difficult to reason about which code depends on which parts of configuration. One of the original design goals was to make it very easy to remove (as well as add) un-needed configuration classes (i.e. for unused/unsuccessful models)\n",
    " - Because configurations are mutable (by design), using them everywhere makes it harder to reason about the state of the system (nothing is \"pure\" or \"const correct\" any more). It would be better to separate the parts of the code that depend on configuration (and are subject to change) from those that do not (i.e. analytics, pure/idempotent functions, etc)\n",
    " "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "eef20b0a-d5a1-4e4c-aac1-c45b0192bab1",
   "metadata": {},
   "source": [
    "### Adding a \\_\\_call__ method\n",
    "One solution to this problem is to write code that lives as \"close\" to the relevant configuration as possible, such that the scope is limited to those configuration parameters it needs and the dependencies are more clear. In this way, the registry is not used at all for run-time access, except perhaps as an initial entry-point into the logic. Furthermore, this code can serve as the bridge between the configuration graph (i.e. `ccflow`) and any other computational graphs which will be doing the heavy lifting. For example, the configurations can be used to define tensor graphs (tensorflow, pytorch), event processing graphs (csp, kafka streams), task graphs (ray, dask), etc, which are then executed separately.\n",
    "\n",
    "The easiest way to bind together user-defined business logic with the configuration classes is simply to add a method on the class. Following the [Single Responsibility Principle](https://en.wikipedia.org/wiki/Single-responsibility_principle), each of these classes should ideally have only one purpose, and hence following the python convention, we can name the primary method that commplishes this `__call__` so that the BaseModel becomes callable. The following examples illustrate this idea."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "587eb550-7c23-48cd-9ae9-5ae1ae2f4383",
   "metadata": {},
   "source": [
    "### Example: Reading a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "894eb33d-3e9e-4312-97dd-81f3d956ec97",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyFileConfig(BaseModel):\n",
    "    \"\"\"This is an example of a config class.\"\"\"\n",
    "\n",
    "    file: Path\n",
    "    description: str = \"N/A\"\n",
    "    version: int = 0\n",
    "\n",
    "    def __call__(self):\n",
    "        \"\"\"Read the file as a pandas data frame\"\"\"\n",
    "        return pd.read_parquet(self.file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "86e76d36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MyFileConfig(file=PosixPath('ccflow/examples/example.parquet'), description='My Data', version=0)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Row ID</th>\n",
       "      <th>Order ID</th>\n",
       "      <th>Order Date</th>\n",
       "      <th>Ship Date</th>\n",
       "      <th>Ship Mode</th>\n",
       "      <th>Customer ID</th>\n",
       "      <th>Segment</th>\n",
       "      <th>Country</th>\n",
       "      <th>City</th>\n",
       "      <th>State</th>\n",
       "      <th>Postal Code</th>\n",
       "      <th>Region</th>\n",
       "      <th>Product ID</th>\n",
       "      <th>Category</th>\n",
       "      <th>Sub-Category</th>\n",
       "      <th>Sales</th>\n",
       "      <th>Quantity</th>\n",
       "      <th>Discount</th>\n",
       "      <th>Profit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>30-1718825</td>\n",
       "      <td>2024-06-20</td>\n",
       "      <td>2024-06-22</td>\n",
       "      <td>Standard Class</td>\n",
       "      <td>GF3 E0D</td>\n",
       "      <td>C</td>\n",
       "      <td>US</td>\n",
       "      <td>Victoriabury</td>\n",
       "      <td>Iowa</td>\n",
       "      <td>50365</td>\n",
       "      <td>Region 4</td>\n",
       "      <td>LWSE88451214620527</td>\n",
       "      <td>Telecommunication Services</td>\n",
       "      <td>Diversified Telecommunication Services</td>\n",
       "      <td>9900</td>\n",
       "      <td>190</td>\n",
       "      <td>47.04</td>\n",
       "      <td>541.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>98-7267735</td>\n",
       "      <td>2024-05-12</td>\n",
       "      <td>2024-06-05</td>\n",
       "      <td>Standard Class</td>\n",
       "      <td>028-OOM</td>\n",
       "      <td>B</td>\n",
       "      <td>US</td>\n",
       "      <td>Melissamouth</td>\n",
       "      <td>Iowa</td>\n",
       "      <td>78255</td>\n",
       "      <td>Region 4</td>\n",
       "      <td>JTSB68523839821087</td>\n",
       "      <td>Materials</td>\n",
       "      <td>Chemicals</td>\n",
       "      <td>1900</td>\n",
       "      <td>510</td>\n",
       "      <td>96.64</td>\n",
       "      <td>266.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>67-9356762</td>\n",
       "      <td>2024-06-07</td>\n",
       "      <td>2024-06-25</td>\n",
       "      <td>First Class</td>\n",
       "      <td>SIR-971</td>\n",
       "      <td>C</td>\n",
       "      <td>US</td>\n",
       "      <td>Madisontown</td>\n",
       "      <td>Vermont</td>\n",
       "      <td>24144</td>\n",
       "      <td>Region 4</td>\n",
       "      <td>OKGX33209192480300</td>\n",
       "      <td>Energy</td>\n",
       "      <td>Energy Equipment &amp; Services</td>\n",
       "      <td>1300</td>\n",
       "      <td>770</td>\n",
       "      <td>23.79</td>\n",
       "      <td>95.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>75-5326684</td>\n",
       "      <td>2024-05-25</td>\n",
       "      <td>2024-05-28</td>\n",
       "      <td>Second Class</td>\n",
       "      <td>939 BJK</td>\n",
       "      <td>B</td>\n",
       "      <td>US</td>\n",
       "      <td>Maxwellville</td>\n",
       "      <td>Wyoming</td>\n",
       "      <td>93403</td>\n",
       "      <td>Region 4</td>\n",
       "      <td>NYXC50908782002016</td>\n",
       "      <td>Materials</td>\n",
       "      <td>Metals &amp; Mining</td>\n",
       "      <td>800</td>\n",
       "      <td>690</td>\n",
       "      <td>10.78</td>\n",
       "      <td>734.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>30-9242910</td>\n",
       "      <td>2024-01-06</td>\n",
       "      <td>2024-06-05</td>\n",
       "      <td>Standard Class</td>\n",
       "      <td>2IJ A87</td>\n",
       "      <td>A</td>\n",
       "      <td>US</td>\n",
       "      <td>Lake Angelashire</td>\n",
       "      <td>Oklahoma</td>\n",
       "      <td>29249</td>\n",
       "      <td>Region 4</td>\n",
       "      <td>SWVG27221019552286</td>\n",
       "      <td>Utilities</td>\n",
       "      <td>Water Utilities</td>\n",
       "      <td>5800</td>\n",
       "      <td>730</td>\n",
       "      <td>37.62</td>\n",
       "      <td>0.68</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Row ID    Order ID  Order Date   Ship Date       Ship Mode Customer ID  \\\n",
       "0       0  30-1718825  2024-06-20  2024-06-22  Standard Class     GF3 E0D   \n",
       "1       1  98-7267735  2024-05-12  2024-06-05  Standard Class     028-OOM   \n",
       "2       2  67-9356762  2024-06-07  2024-06-25     First Class     SIR-971   \n",
       "3       3  75-5326684  2024-05-25  2024-05-28    Second Class     939 BJK   \n",
       "4       4  30-9242910  2024-01-06  2024-06-05  Standard Class     2IJ A87   \n",
       "\n",
       "  Segment Country              City     State Postal Code    Region  \\\n",
       "0       C      US      Victoriabury      Iowa       50365  Region 4   \n",
       "1       B      US      Melissamouth      Iowa       78255  Region 4   \n",
       "2       C      US       Madisontown   Vermont       24144  Region 4   \n",
       "3       B      US      Maxwellville   Wyoming       93403  Region 4   \n",
       "4       A      US  Lake Angelashire  Oklahoma       29249  Region 4   \n",
       "\n",
       "           Product ID                    Category  \\\n",
       "0  LWSE88451214620527  Telecommunication Services   \n",
       "1  JTSB68523839821087                   Materials   \n",
       "2  OKGX33209192480300                      Energy   \n",
       "3  NYXC50908782002016                   Materials   \n",
       "4  SWVG27221019552286                   Utilities   \n",
       "\n",
       "                             Sub-Category  Sales  Quantity  Discount  Profit  \n",
       "0  Diversified Telecommunication Services   9900       190     47.04  541.33  \n",
       "1                               Chemicals   1900       510     96.64  266.85  \n",
       "2             Energy Equipment & Services   1300       770     23.79   95.39  \n",
       "3                         Metals & Mining    800       690     10.78  734.79  \n",
       "4                        Water Utilities    5800       730     37.62    0.68  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "config = MyFileConfig(\n",
    "    file=\"ccflow/examples/example.parquet\", description=\"My Data\"\n",
    ")\n",
    "df = config()\n",
    "print(config)\n",
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4cf8ac45-dd8e-4c76-bb91-899257188684",
   "metadata": {},
   "source": [
    "Thus, using all the machinery in the previous section, we can define configs that are either pure data containers, or that correspond to some piece of arbitrary user-defined functionality, i.e. a \"step\" in a workflow. While we do leverage pydantic for conforming data, there are **no restrictions** on the kind of code that could live inside the `__call__` method (or strictly speaking, how many methods there are or what those methods are called). Here is a slightly more complex example for illustration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a199941a-9bd5-4de5-923c-cc542a455105",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow.parquet as pq\n",
    "\n",
    "\n",
    "class MyFileConfig(BaseModel):\n",
    "    \"\"\"This is an example of a config class.\"\"\"\n",
    "\n",
    "    file: Path\n",
    "    description: str = \"N/A\"\n",
    "    version: int = 0\n",
    "\n",
    "    def read_pandas(self):\n",
    "        return pd.read_parquet(self.file)\n",
    "\n",
    "    def read_arrow(self):\n",
    "        return pq.read_table(self.file)\n",
    "\n",
    "    def __call__(self):\n",
    "        \"\"\"Read the file as a pandas data frame or arrow table\"\"\"\n",
    "        if self.version == 0:\n",
    "            return self.read_pandas()\n",
    "        else:\n",
    "            return self.read_arrow()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6426d86d-193a-4118-85a7-1d3325a2e119",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "<class 'pyarrow.lib.Table'>\n"
     ]
    }
   ],
   "source": [
    "config = MyFileConfig(file=\"ccflow/examples/example.parquet\", version=0)\n",
    "df = config()\n",
    "print(type(df))\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "config = MyFileConfig(file=\"ccflow/examples/example.parquet\", version=1)\n",
    "df = config()\n",
    "print(type(df))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c47c212a-6281-4dcf-90e0-31628f7a3933",
   "metadata": {},
   "source": [
    "### Example: Custom publisher"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1991a21a-370c-46f7-8949-ffd5e5443e2b",
   "metadata": {},
   "source": [
    "A common use case in any research/production framework is to send data from the current process to some other location. In extract-transform-load (ETL) processes, the target location is usually files, an object store or a database. However, automation of research reports containing tables, charts and HTML is another common use case; those may be written to files, sent to an experiment tracking framework or simply emailed to a set of recipients.\n",
    "\n",
    "In `ccflow.flow`, we used the principles above to define a very simple interface for \"publishers\" (`ccflow.flow.publishers.BasePublisher`), which are configurable components who know how to take data from one place and send it to another. The reason for having a `BasePublisher` class is so that publishers can easily be substituted for one another as part of the configuration of a larger workflow. If publishers were all implemented a little bit differently, then it would be difficult to switch from, i.e. writing files to sending an email purely based on configuration.\n",
    "\n",
    "While `ccflow.flow` provides several implementations out of the box for common use cases (files, email, [mlflow](https://www.mlflow.org/docs/latest/tracking.html)), custom implementations of the interface are also straightforward. Below we will create a custom publisher that uses IPython's \"display\" function to display a list of strings as html. We use Jinja templating as described in a previous section to define the html template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a9a92f25-e7d3-4190-b866-bacf6160e690",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<b>My test publisher:</b>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<p style=\"color:blue;\">Blue text.<BR>More blue text.</p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "from ccflow import JinjaTemplate, BasePublisher\n",
    "from typing import List\n",
    "\n",
    "\n",
    "class MyPublisher(BasePublisher):\n",
    "    data: List[str] = None\n",
    "    html_template: JinjaTemplate\n",
    "\n",
    "    def __call__(self):\n",
    "        display(HTML(self.get_name()))\n",
    "        display(HTML(self.html_template.template.render(data=\"<BR>\".join(self.data))))\n",
    "\n",
    "\n",
    "# Create the publisher (i.e. via static configuration)\n",
    "p = MyPublisher(\n",
    "    name=\"<b>My {{desc}} publisher:</b>\",\n",
    "    html_template=\"\"\"<p style=\"color:blue;\">{{data}}</p>\"\"\",\n",
    ")\n",
    "\n",
    "# Set the data that we want to publish (i.e. at runtime)\n",
    "p.name_params = dict(desc=\"test\")\n",
    "p.data = [\"Blue text.\", \"More blue text.\"]\n",
    "p()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "43146964-b0b3-43af-ba82-41db1cdaa911",
   "metadata": {},
   "source": [
    "Even though \"data\" is a standard attribute on BasePublisher, implementations can override it to define more specific data types that the publisher allows, and pydantic provides the validation. For example, passing a dictionary to \"data\" in the example above results in an error (before the call to publish is even made):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "5daae59f-6cee-49db-ab0e-d0f3780c9157",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 validation error for MyPublisher\n",
      "data\n",
      "  Input should be a valid list [type=list_type, input_value={}, input_type=dict]\n",
      "    For further information visit https://errors.pydantic.dev/2.7/v/list_type\n"
     ]
    }
   ],
   "source": [
    "p = MyPublisher(\n",
    "    name=\"<b>My {{desc}} publisher:</b>\",\n",
    "    html_template=\"\"\"<p style=\"color:blue;\">{{data}}</p>\"\"\",\n",
    ")\n",
    "try:\n",
    "    p.data = {}\n",
    "except ValueError as v:\n",
    "    print(v)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "28258ebf-4858-43f9-82a0-1219fbcdffb3",
   "metadata": {},
   "source": [
    "### Example: Gaussian process regression in sklearn"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e9039e39-90b4-44c0-93e9-3f00ddb5969c",
   "metadata": {},
   "source": [
    "Gaussian Processes (GP) are a generic supervised learning method designed to solve non-linear regression and probabilistic classification problems. The GaussianProcessRegressor in `sklearn` implements Gaussian processes (GP) for regression purposes. For this, the prior of the GP needs to be specified. The prior mean is assumed to be constant and zero (for normalize_y=False) or the training datas mean (for normalize_y=True). The priors covariance is specified by passing a kernel object.\n",
    "\n",
    "In the example below, we should how `ccflow` can be used to configure a GP Regression (including the kernel object). Even though the kernel object is not a pydantic type, we can still configure it in this framework. \n",
    "\n",
    "Note that `sklearn` must be installed to run this example. For the meaning of the parameters, refer to the [sklearn documentation](https://scikit-learn.org/stable/modules/generated/sklearn.gaussian_process.GaussianProcessRegressor.html#sklearn.gaussian_process.GaussianProcessRegressor); we follow their example for usage of the GP Regressor.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ea584355-b3ca-4f8e-a83f-d46732c34d10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.36802938610173386\n",
      "-1.439871997881387\n"
     ]
    }
   ],
   "source": [
    "%%capture --no-stdout\n",
    "try:\n",
    "    from sklearn.datasets import make_friedman2\n",
    "    from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "    from sklearn.gaussian_process.kernels import Kernel\n",
    "    from typing import Callable, Union\n",
    "    from hydra.utils import instantiate\n",
    "\n",
    "    class GPRegressionModel(BaseModel):\n",
    "        \"\"\"Wrapping of sklearn's GaussianProcessRegressor for configuration\"\"\"\n",
    "\n",
    "        kernel: Kernel\n",
    "        alpha: float = 1e-10\n",
    "        optimizer: Union[str, Callable] = \"fmin_l_bfgs_b\"\n",
    "        n_restarts_optimizer: int = 0\n",
    "        normalize_y: bool = False\n",
    "        random_state: int = None\n",
    "\n",
    "        class Config:\n",
    "            # Here we tell pydantic to allow the \"Kernel\" type, which is not a standard pydantic type\n",
    "            arbitrary_types_allowed = True\n",
    "\n",
    "        def __call__(self):\n",
    "            \"\"\"Build the GP Regressor object\"\"\"\n",
    "            # Rather than passing in each attribute individually, we can use the dict representation of the config class (leaving out the \"type_\" attribute),\n",
    "            # as we named everything consistently with the sklearn parameter names\n",
    "            return GaussianProcessRegressor(**self.dict(exclude={\"type_\"}))\n",
    "\n",
    "    # Define the config as a dictionary (potentially to live in a config file)\n",
    "    gpr_config = {\n",
    "        \"_target_\": \"__main__.GPRegressionModel\",\n",
    "        \"kernel\": {\n",
    "            \"_target_\": \"sklearn.gaussian_process.kernels.Sum\",\n",
    "            \"k1\": {\n",
    "                \"_target_\": \"sklearn.gaussian_process.kernels.DotProduct\",\n",
    "                \"sigma_0\": 1.0,\n",
    "            },\n",
    "            \"k2\": {\n",
    "                \"_target_\": \"sklearn.gaussian_process.kernels.WhiteKernel\",\n",
    "                \"noise_level\": 1.0,\n",
    "            },\n",
    "        },\n",
    "        \"optimizer\": \"fmin_l_bfgs_b\",\n",
    "        \"random_state\": 0,\n",
    "    }\n",
    "\n",
    "    # Load the config in the root registry\n",
    "    root = ModelRegistry.root().clear()\n",
    "    root.load_config({\"gpr\": gpr_config})\n",
    "\n",
    "    # Use the config to train the configured model\n",
    "    X, y = make_friedman2(n_samples=500, noise=0, random_state=0)\n",
    "    gpr = root[\"gpr\"]().fit(X, y)\n",
    "    print(gpr.score(X, y))\n",
    "\n",
    "    # We can now change the config interactively to experiment with different kernels,\n",
    "    # without needing to go back to config dictionaries or files\n",
    "    from sklearn.gaussian_process.kernels import RBF, WhiteKernel\n",
    "\n",
    "    root[\"gpr\"].kernel = RBF() + WhiteKernel()\n",
    "    gpr = root[\"gpr\"]().fit(X, y)\n",
    "    print(gpr.score(X, y))\n",
    "\n",
    "except ImportError:\n",
    "    print(\"sklearn must be installed to run this example\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e37b2bcf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
